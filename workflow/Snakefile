# Main entrypoint of the workflow.
# Please follow best practices: 
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html

import pandas as pd
import yaml
import os

DEBUG = False
version = "0.3.36"

def debug(msg):
    if DEBUG:
        print(f"[DEBUG] {msg}")

debug("Starting workflow setup...")
debug(f"Workflow version: {version}")

# Load configuration files
debug("Loading configuration files...")

debug(f"Reading: {config['params-file']}")
params = yaml.load(open(config["params-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading: {config['docker-file']}")
docker = yaml.load(open(config["docker-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading: {config['features-file']}")
features = yaml.load(open(config["features-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading sample metadata: {config['sample-file']}")
samples = pd.read_table(config["sample-file"], comment="#", dtype="str")

# Process sample information
debug("Processing sample metadata...")

samples = (
    samples.assign(assembly_id=samples.assembly_ids.str.replace(" ", "").str.split(","))
    .explode("assembly_id")
    .sort_values(by=["assembly_id", "sample_id", "library_id"])
)
samples = samples.assign(assembly_id=samples.assembly_id.str.strip())

debug("Generating global variables from sample metadata...")

SAMPLES = samples.sample_id.unique()
debug(f"Found {len(SAMPLES)} unique samples.")

SAMPLE_LIBRARY = samples[["sample_id", "library_id"]].values.tolist()
debug(f"Generated sample-library pairs: {len(SAMPLE_LIBRARY)}")

ASSEMBLY_SAMPLE_LIBRARY = (
    samples[["assembly_id", "sample_id", "library_id"]].dropna(axis=0).values.tolist()
)
debug(f"Generated assembly-sample-library triplets: {len(ASSEMBLY_SAMPLE_LIBRARY)}")

ASSEMBLIES = [
    assembly_id
    for assembly_id in samples.assembly_id.unique()
    if not pd.isna(assembly_id)
]
debug(f"Total assemblies: {len(ASSEMBLIES)}")

# Load paths and features
debug("Loading pipeline-specific paths and feature settings...")

KRAKEN2SHM = config["kraken_tmp_storage"]
DIAMONDSHM = config["nvme_storage"]
KRAKEN2_DBS = features["databases"]["kraken2"]
METAPHLAN_DBS = features["databases"]["metaphlan4"]
PHYLOFLASH_DBS = features["databases"]["phyloflash"]
HOST_NAMES = [] if features["hosts"] is None else list(features["hosts"].keys())
LAST_HOST = HOST_NAMES[-1] if HOST_NAMES else None

debug(f"KRAKEN2 temp storage: {KRAKEN2SHM}")
debug(f"DIAMOND temp storage: {DIAMONDSHM}")
debug(f"Host names: {HOST_NAMES}, last host: {LAST_HOST}")

READS_R = os.path.join(config["pipeline_folder"], "workflow/scripts/R/reads.Rmd")
PREPROCESS_R = os.path.join(config["pipeline_folder"], "workflow/scripts/R/preprocess.Rmd")

debug(f"R script for reads: {READS_R}")
debug(f"R script for preprocessing: {PREPROCESS_R}")

# Import subworkflows
debug("Including subworkflows...")

include: "rules/folders.smk"
include: "rules/helpers/__main__.smk"
include: "rules/reads/__main__.smk"
include: "rules/reference/__main__.smk"
include: "rules/preprocess/__main__.smk"
include: "rules/assemble/__main__.smk"
include: "rules/quantify/__main__.smk"
include: "rules/annotate/__main__.smk"
include: "rules/contig_annotate/__main__.smk"
include: "rules/report/__main__.smk"

debug("Subworkflows included. Ready to run.")

rule all:
    """Run the entire pipeline"""
    input:
        rules.reads.input,
        rules.preprocess.input,
        rules.assemble.input,
        rules.annotate.input,
        rules.contig_annotate.input,
        rules.report.input,
