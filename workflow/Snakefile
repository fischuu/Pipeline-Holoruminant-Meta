# Main entrypoint of the workflow.
# Please follow best practices: 
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html

import pandas as pd
import yaml
import os

DEBUG = True
version = "0.4.4"

def debug(msg):
    if DEBUG:
        print(f"[DEBUG] {msg}")

debug("Starting workflow setup...")
debug(f"Workflow version: {version}")

# Load configuration files
debug("Loading configuration files...")

debug(f"Reading: {config['params-file']}")
params = yaml.load(open(config["params-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading: {config['docker-file']}")
docker = yaml.load(open(config["docker-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading: {config['features-file']}")
features = yaml.load(open(config["features-file"], "r"), Loader=yaml.SafeLoader)

debug(f"Reading sample metadata: {config['sample-file']}")
samples = pd.read_table(config["sample-file"], comment="#", dtype="str")

# Process sample information
debug("Processing sample metadata...")

samples = (
    samples.assign(assembly_id=samples.assembly_ids.str.replace(" ", "").str.split(","))
    .explode("assembly_id")
    .sort_values(by=["assembly_id", "sample_id", "library_id"])
)
samples = samples.assign(assembly_id=samples.assembly_id.str.strip())

debug("Generating global variables from sample metadata...")

SAMPLES = samples.sample_id.unique()
debug(f"Found {len(SAMPLES)} unique samples.")

SAMPLE_LIBRARY = samples[["sample_id", "library_id"]].values.tolist()
debug(f"Generated sample-library pairs: {len(SAMPLE_LIBRARY)}")

ASSEMBLY_SAMPLE_LIBRARY = (
    samples[["assembly_id", "sample_id", "library_id"]].dropna(axis=0).values.tolist()
)
debug(f"Generated assembly-sample-library triplets: {len(ASSEMBLY_SAMPLE_LIBRARY)}")

ASSEMBLIES = [
    assembly_id
    for assembly_id in samples.assembly_id.unique()
    if not pd.isna(assembly_id)
]
debug(f"Total assemblies: {len(ASSEMBLIES)}")

# Load paths and features
debug("Loading pipeline-specific paths and feature settings...")

def choose_temp(primary, fallback1=None, fallback2=None):
    """Choose a temporary storage path with fallbacks based on non-empty strings."""
    for path in [primary, fallback1, fallback2]:
        if path != "":  # check if string is not empty
            return path
    raise RuntimeError("No temporary storage available!")


# Fallback chain: shm -> nvme -> tmp
KRAKEN2SHM = choose_temp(config["shm_storage"], config["nvme_storage"], config["tmp_storage"])
KRAKEN2NVME = config["nvme_storage"]
DIAMONDSHM = choose_temp(config["shm_storage"], config["nvme_storage"], config["tmp_storage"])
DIAMONDNVME = config["nvme_storage"]
EGGNOGSHM = choose_temp(config["shm_storage"], config["nvme_storage"], config["tmp_storage"])
EGGNOGNVME = config["nvme_storage"]

#KRAKEN2SHM = config["kraken_tmp_storage"]
#KRAKEN2SHM = config["shm_storage"]
#DIAMONDSHM = config["nvme_storage"]
#EGGNOGSHM = config["shm_storage"]
KRAKEN2_DBS = features["databases"]["kraken2"]
METAPHLAN_DBS = features["databases"]["metaphlan4"]
PHYLOFLASH_DBS = features["databases"]["phyloflash"]
HOST_NAMES = [] if features["hosts"] is None else list(features["hosts"].keys())
LAST_HOST = HOST_NAMES[-1] if HOST_NAMES else None

debug(f"KRAKEN2 fast temp storage: {KRAKEN2SHM}")
debug(f"KRAKEN2 fall-back temp storage: {KRAKEN2NVME}")
debug(f"DIAMOND fast temp storage: {DIAMONDSHM}")
debug(f"EGGNOG fast temp storage: {EGGNOGSHM}")
debug(f"Host names: {HOST_NAMES}, last host: {LAST_HOST}")

READS_R = os.path.join(config["pipeline_folder"], "workflow/scripts/R/reads.Rmd")
PREPROCESS_R = os.path.join(config["pipeline_folder"], "workflow/scripts/R/preprocess.Rmd")

debug(f"R script for reads: {READS_R}")
debug(f"R script for preprocessing: {PREPROCESS_R}")

# Import the escalation orders
import yaml

# --- Load one-line comma-separated ESCALATION config ---
with open("config/escalation.yaml") as f:
    raw = yaml.safe_load(f)

ESCALATION_CONFIG = {k: [x.strip() for x in v.split(",")] for k, v in raw.items()}

# Import subworkflows
debug("Including subworkflows...")

include: "rules/folders.smk"
include: "rules/helpers/__main__.smk"
include: "rules/reads/__main__.smk"
include: "rules/reference/__main__.smk"
include: "rules/preprocess/__main__.smk"
include: "rules/read_annotate/__main__.smk"
include: "rules/assemble/__main__.smk"
include: "rules/quantify/__main__.smk"
include: "rules/mag_annotate/__main__.smk"
include: "rules/contig_annotate/__main__.smk"
include: "rules/report/__main__.smk"
include: "rules/devel/throw_error.smk"

debug("Subworkflows included. Ready to run.")

rule all:
    """Run the entire pipeline"""
    input:
        rules.reads.input,
        rules.preprocess.input,
        rules.read_annotate.input,
        rules.assemble.input,
        rules.mag_annotate.input,
        rules.contig_annotate.input,
        rules.quantify.input,
  #      rules.report.input,
