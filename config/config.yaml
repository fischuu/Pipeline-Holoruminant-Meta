#################################
##
## File configuration configuration

pipeline_folder: "/users/fischerd/git/Pipeline-Holoruminant-Meta/"

features-file: "config/features.yaml"
params-file: "config/params.yaml"
docker-file: "config/.docker.yml"
sample-file: "config/samples.tsv"
dram-config: "config/dram_config.txt"
proteinortho-config: "config/proteinortho_config.txt"
phylophlan-config: "config/phylophlan/supertree_aa.cfg"

kraken_tmp_storage: "$LOCAL_SCRATCH"     # This entry should be removed, once shm is introduced everywhere
tmp_storage: "$TMP"
nvme_storage: "$LOCAL_SCRATCH"
shm_storage: "/dev/shm"      # If shm is not available, set to a fast disc (same as nvme_storage), if that is not available, point it to a regular disc (like tmp_storage)

assembler: "metaspades"  # "metaspades" or "megahit". Megahit serves as fall-back in case metaspades cannot create the assemblies

#################################
##
## Server configuration

resources:
# Time allocations in minutes, for rules that run only short, long or very long 
    time:
        shortrun: 300 
        longrun: 4000
        verylongrun: 20000
# Memory allocations in MB for different profiles
    mem_per_cpu:
        krakenmem: 140000  # We use /dev/shm, make sure there is enough RAM on node to host the database! between 100GB and 1.5TB!
        veryhighmem: 1400000
        quitehighmem: 320000
        highmem: 160000
        lowmem: 16000
# CPU settings for single thread and multi thread jobs
    cpu_per_task:
        single_thread: 1
        multi_thread: 24
        kraken_thread: 20   # We use /dev/shm, which can consume the nodes RAM without safeguard, so make sure no other job runs on the node!!! For that, allocate all cpus from the node where kraken will run
# Partition names for slurm executor
    partition:
        testing: "test"
        small: "small"
        kraken: "small"
        highmem: "hugemem"
        longrun: "longrun"
        highlong: "hugemem_longrun"
# If your system supports fast nvme disc, set this values, otherwise set to 0 (not tested, as our system has nvme discs!)
    nvme:
        small: 10
        large: 50
        verylarge: 250
        kraken: 250
        
        
## Below this line here is the new resource allocations. We will work with predefined onfiguration modules that can be escalated during retries

resource_sets:
  small:
    runtime: 300
    mem_mb: 16000
    cpus: 1
    partition: "small"
    nvme: 0

  medium:
    runtime: 4000
    mem_mb: 160000
    cpus: 10
    partition: "small"
    nvme: 10
    
  large:
    runtime: 4000
    mem_mb: 320000
    cpus: 20
    partition: "small"
    nvme: 50

  highmem:
    runtime: 4000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem"
    nvme: 250

  longrun:
    runtime: 20000
    mem_mb: 320000
    cpus: 20
    partition: "longrun"
    nvme: 250

  highmem_longrun:
    runtime: 20000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem_longrun"
    nvme: 250

 # This node is used for setting up a ramdisc for time-critical tasks. Make sure, that the allocation ensures that the entire node is
 # requested for the job so that there won't be other users on the same node while the job runs. Their jobs might get affected by the
 # /dev/shm access that is happening for rules using this resource_set!
 # In case you do not want to (or cannot) use /dev/shm please define a different file path in config/config.yaml under "shm_storage"
  full_node:
    runtime: 4000
    mem_mb: 360000
    cpus: 40
    partition: "small"
    nvme: 250