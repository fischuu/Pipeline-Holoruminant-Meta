#################################
##
## File configuration configuration

pipeline_folder: "/users/fischerd/git/Snakebite-Holoruminant-MetaG/"

features-file: "config/features.yaml"
params-file: "config/params.yaml"
docker-file: "config/.docker.yml"
sample-file: "config/samples.tsv"
dram-config: "config/dram_config.txt"
proteinortho-config: "config/proteinortho_config.txt"
phylophlan-config: "config/phylophlan/supertree_aa.cfg"

tmp_storage: "$TMP"
nvme_storage: "$LOCAL_SCRATCH"
shm_storage: "/dev/shm"      # If shm is not available, set to a fast disc (same as nvme_storage), if that is not available, point it to a regular disc (like tmp_storage)

copy_dbs: "True"          # If "True", dbs will be copied to faster discspaces, where applicable. Set to "False" to keep the original locations during runtime.
diamond_shm: "True"       # Set "True", if shm space should be used for diamond, if "False" nvme_storage will be used as fall back (if copy_dbs="True")
kraken2_shm: "True"       # Set "True", if shm space should be used for diamond, if "False" nvme_storage will be used as fall back (if copy_dbs="True")

assembler: "metaspades"  # "metaspades", "megahit" or e.g. "provided". Megahit serves as fall-back in case metaspades cannot create the assemblies. With any other folder name the pipeline expects self-provided assemblies in results/assemble/<name>/<assembly>.fa.gz .


#################################
##
## Server configuration

resource_sets:
  small:
    runtime: 300
    mem_mb: 16000
    cpus: 1
    partition: "small"
    nvme: "nvme:0"

  medium:
    runtime: 4000
    mem_mb: 160000
    cpus: 10
    partition: "small"
    nvme: "nvme:0"

  medium_nvme:
    runtime: 4000
    mem_mb: 160000
    cpus: 10
    partition: "small"
    nvme: "nvme:10"    
    
  large:
    runtime: 4000
    mem_mb: 320000
    cpus: 20
    partition: "small"
    nvme: "nvme:0"

  large_nvme:
    runtime: 4000
    mem_mb: 320000
    cpus: 20
    partition: "small"
    nvme: "nvme:50"

  highmem:
    runtime: 4000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem"
    nvme: "nvme:250"

  longrun:
    runtime: 20000
    mem_mb: 320000
    cpus: 20
    partition: "longrun"
    nvme: "nvme:250"

  highmem_longrun:
    runtime: 20000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem_longrun"
    nvme: "nvme:250"

  full_node:
    runtime: 4000
    mem_mb: 360000
    cpus: 40
    partition: "small"
    nvme: "nvme:1000"
    
 # The previous node is used for setting up a ramdisc for time-critical tasks. Make sure, that the allocation ensures that the entire node is
 # requested for the job so that there won't be other users on the same node while the job runs. Their jobs might get affected by the
 # /dev/shm access that is happening for rules using this resource_set!
 # In case you do not want to (or cannot) use /dev/shm please define a different file path in config/config.yaml under "shm_storage"
