#################################
##
## File configuration configuration

pipeline_folder: "/users/fischerd/git/Pipeline-Holoruminant-Meta/"

features-file: "config/features.yaml"
params-file: "config/params.yaml"
docker-file: "config/.docker.yml"
sample-file: "config/samples.tsv"
dram-config: "config/dram_config.txt"
proteinortho-config: "config/proteinortho_config.txt"
phylophlan-config: "config/phylophlan/supertree_aa.cfg"

tmp_storage: "$TMP"
nvme_storage: "$LOCAL_SCRATCH"
shm_storage: "/dev/shm"      # If shm is not available, set to a fast disc (same as nvme_storage), if that is not available, point it to a regular disc (like tmp_storage)

diamond_shm: "True"       # Set here, if shm space should be used for diamond, otherwise nvme will be used as fall back
kraken2_shm: "True"       # Set here, if shm space should be used for diamond, otherwise nvme will be used as fall back

assembler: "metaspades"  # "metaspades" or "megahit". Megahit serves as fall-back in case metaspades cannot create the assemblies

#################################
##
## Server configuration

resource_sets:
  small:
    runtime: 300
    mem_mb: 16000
    cpus: 1
    partition: "small"
    nvme: "nvme:0"

  medium:
    runtime: 4000
    mem_mb: 160000
    cpus: 10
    partition: "small"
    nvme: "nvme:10"
    
  large:
    runtime: 4000
    mem_mb: 320000
    cpus: 20
    partition: "small"
    nvme: "nvme:50"

  highmem:
    runtime: 4000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem"
    nvme: "nvme:250"

  longrun:
    runtime: 20000
    mem_mb: 320000
    cpus: 20
    partition: "longrun"
    nvme: "nvme:250"

  highmem_longrun:
    runtime: 20000
    mem_mb: 1400000
    cpus: 20
    partition: "hugemem_longrun"
    nvme: "nvme:250"

  full_node:
    runtime: 4000
    mem_mb: 360000
    cpus: 40
    partition: "small"
    nvme: "nvme:1000"
    
 # The previous node is used for setting up a ramdisc for time-critical tasks. Make sure, that the allocation ensures that the entire node is
 # requested for the job so that there won't be other users on the same node while the job runs. Their jobs might get affected by the
 # /dev/shm access that is happening for rules using this resource_set!
 # In case you do not want to (or cannot) use /dev/shm please define a different file path in config/config.yaml under "shm_storage"
